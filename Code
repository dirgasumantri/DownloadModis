# -*- coding: utf-8 -*-
"""
Created on Wed Jan  9 13:00:34 2019

@author: 
"""
import numpy as np
import urllib3
import urllib.request
import asyncio
import optparse
import os
try:
    import urllib.request as urllib2
except ImportError:
    import urllib2
import time
import calendar
import logging
import sys
import fnmatch
import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning

requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

import urllib.request as req

__author__ = "J Gomez-Dans"
__copyright__ = "Copyright 2013-2017 J Gomez-Dans"
__version__ = "1.3.3"
__license__ = "GPLv3"
__email__ = "j.gomez-dans@ucl.ac.uk"

HEADERS = { 'User-Agent' : 'get_modis Python %s' % __version__ }
CHUNKS = 65536


#proxy = { "http" : "http://167.205.22.103:8080" } #without proxy authentication
proxy_url = { 'http' : r'http://username:password@167.205.00.000:8080',
             'https' : r'https://username:password@167.205.00.000:8080',
             'ftp' : r'ftp://username:password@167.205.00.000:8080' } #with Proxy Authentication
#proxy = urllib2.ProxyHandler(proxy_url)
#auth = urllib2.HTTPBasicAuthHandler()
#opener = urllib2.build_opener(proxy, auth, urllib2.HTTPHandler)
#        urllib2.install_opener(opener)

#    import requests
#proxies = {
#    "http": "http://username:password@167.205.00.000:8080/",
#}
#ready for `req = requests.get(url, proxies=proxies)


#proxy = req.ProxyHandler(proxy_url)
#auth = req.HTTPBasicAuthHandler()
#opener = req.build_opener(proxy, auth, req.HTTPHandler)
#req.install_opener(opener)

#pakai urlretrieve -> bisa -> tinggal autentication proxy
# dg sdh didefinisikan proxy, auth, opener, maka bisa dilakukan:
#  urlopen atau urlretrieve

#conn = req.urlopen('http://www.google.com')
#return_str = conn.read()
#print(return_str)
username = 'username'
password = 'password#'
#the_url = 'https://e4ftl01.cr.usgs.gov//MODV6_Dal_H/MOTA/MCD19A2.006/2018.12.31/MCD19A2.A2018365.h05v10.006.2019009045849.hdf'

try:
    #fileList=np.loadtxt('fileList.txt',dtype='str',delimiter='\t')
    fileList=np.loadtxt('download.txt',dtype='str',delimiter='\t')
except:
    print('Did not find a text file containing file names (perhaps name does not match)')
    sys.exit()
try:
    numOfFiles=len(fileList)
except:
    print('Please use the ARSET module titled "whatever it is", aimed at gridding a single MODIS file.')
    sys.exit()

fileNumber=0
#FILE_NAME=str(fileList[fileNumber])[2:-1]
FILE_NAME=str(fileList[fileNumber])[0:] #nama file kepotong 2 huruf terdepan

for file in range(numOfFiles):
    the_url=str(fileList[file])[0:] #nama file kepotong 2 huruf terdepan
    #FILE_NAME=str(fileList[file])[2:-1] #nama file kepotong 2 huruf terdepan
#    try:
#        # open the hdf file for reading
#        hdf=SD(FILE_NAME)
#    except:
#        print('Unable to open file: \n' + FILE_NAME + '#' + '\n Skipping...')
#        continue    

    with requests.Session() as s:
        s.auth = (username, password)
        r1 = s.request('get', the_url,proxies=proxy_url)
        r = s.get(r1.url, stream=True,proxies=proxy_url)
    
        if not r.ok:
            raise IOError("Can't start download... [%s]" % the_url)
        file_size = int(r.headers['content-length'])
        #fname = the_url.split("/")[-1]
        date = the_url[58:68]
        fname = the_url[68:-4]+('_')+(date)+('.hdf'
                       )
    #    LOG.info("Starting download on %s(%d bytes) ..." %
    #                 (os.path.join(out_dir, fname), file_size))
        with open(fname, 'wb') as fp:
            for chunk in r.iter_content(chunk_size=CHUNKS):
                if chunk:
                    fp.write(chunk)
            fp.flush()
            os.fsync(fp)
    print('Download file : {0}'.format(fname))

#url = 'https://upload.wikimedia.org/wikipedia/commons/b/b0/Kota_Kinabalu_International_Airport.jpg'
#urllib.request.urlretrieve(url, 'kota.jpg')
# url = 'https://e4ftl01.cr.usgs.gov//MODV6_Dal_H/MOTA/MCD19A2.006/2018.12.31/MCD19A2.A2018365.h28v09.006.2019009050058.hdf'

# urllib.request.urlretrieve(url, 'MCD19A2.A2018365.h28v09.006.2019009050058.hdf')




# =============================================================================
# proxy = { "http" : "http://167.205.22.103:8080" }
# get_proxies = urllib.request.getproxies()
#
# urllib.request.AbstractBasicAuthHandler()
# urllib.request.urlopen("http://www.google.com").read()
# =============================================================================

#urllib3.ProxyManager(,proxy)
#urllib.urlopen("http://www.google.com").read()


# =============================================================================
# async def test_tcp(proxy_uri):
#     conn = pproxy.Connection(proxy_uri)
#     reader, writer = await conn.tcp_connect('google.com', 80)
#     writer.write(b'GET / HTTP/1.1\r\n\r\n')
#     data = await reader.read(1024*16)
#     print(data.decode())
#
# #asyncio.run(test_tcp('ss://aes-256-cfb:password@remote_host:remote_port'))
# asyncio.run(test_tcp('ss://abh:51876807@51876807:8080'))
#
#
# =============================================================================

#set HTTP_PROXY= DOMAIN\User_Name:Passw0rd123@PROXY_SERVER_NAME_OR_IP:PORT#
# =============================================================================
# HTTP_PROXY= 'DOMAIN\abh:51876807@167.205.22.103:8080'
#
# print('Beginning file download with urllib2...')
# #urllib.request.HTTPPasswordMgr()
#
# #pakai urlretrieve -> bisa -> tinggal autentication proxy
# url = 'https://matplotlib.org/basemap/_images/sinu.png'
# urllib.request.urlretrieve(url, '/Users/abh/Downloads/sinu.png')
#
# =============================================================================

# str_coord_LowerRightMtrs = str_LowerRightMtrs.split(',')
